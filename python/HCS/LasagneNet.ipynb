{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 'Mon Oct  5 22:51:59 2015')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano.sandbox.cuda\n",
    "import time\n",
    "theano.sandbox.cuda.use(\"gpu1\"), time.asctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import skimage\n",
    "skimage.__version__ # We need at least version 0.11.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplot\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in 10.4165420532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((52950,), (52950,), (52950, 5, 72, 72), numpy.ndarray)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "npzfile = np.load('HCS_72x72.npz')\n",
    "start = time.time()\n",
    "cell_rows = npzfile['arr_0']\n",
    "X = npzfile['arr_1']\n",
    "Y = npzfile['arr_2']\n",
    "print (\"Loaded data in \" + str(time.time() - start))\n",
    "np.shape(cell_rows), np.shape(Y), np.shape(X), type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xmean = X.mean(axis = 0)\n",
    "XStd = np.sqrt(X.var(axis=0))\n",
    "X = (X-Xmean)/(XStd + 0.01)\n",
    "Y = np.asarray(Y,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2854, 5, 72, 72), (2854,), (50096, 5, 72, 72), (50096,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_test = np.asarray(np.recfromtxt ('test_set_data.csv'))\n",
    "idx_test\n",
    "X_train = X[idx_test== False]\n",
    "X_test = X[idx_test]\n",
    "Y_train = Y[idx_test== False]\n",
    "Y_test = Y[idx_test]\n",
    "X_test.shape, Y_test.shape, X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PIXELS = 72\n",
    "conv = (3,3)\n",
    "stride = (1,1)\n",
    "pool = (2,2)\n",
    "\n",
    "num1 = 32\n",
    "num2 = 64\n",
    "num3 = 128\n",
    "p_drop = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Lasagne-0.1dev-py2.7.egg/lasagne/init.py:86: UserWarning: The uniform initializer no longer uses Glorot et al.'s approach to determine the bounds, but defaults to the range (-0.01, 0.01) instead. Please use the new GlorotUniform initializer to get the old behavior. GlorotUniform is now the default for all layers.\n",
      "  warnings.warn(\"The uniform initializer no longer uses Glorot et al.'s \"\n"
     ]
    }
   ],
   "source": [
    "from lasagne import layers\n",
    "from lasagne import nonlinearities\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "input_var = T.tensor4('inputs') #This is a variable needed \n",
    "l_in = lasagne.layers.InputLayer(shape=(None, 5, PIXELS, PIXELS), input_var=input_var) #None depend on batch size\n",
    "\n",
    "\n",
    "conv11 = layers.Conv2DLayer(l_in, num_filters=num1, filter_size=conv)\n",
    "conv11 = layers.Conv2DLayer(conv11, num_filters=num1, filter_size=conv)\n",
    "pool1 = layers.MaxPool2DLayer(conv11, pool_size=pool)\n",
    "\n",
    "conv21 = layers.Conv2DLayer(pool1, num_filters=num2, filter_size=conv)\n",
    "conv22 = layers.Conv2DLayer(conv21, num_filters=num2, filter_size=conv)\n",
    "pool2 = layers.MaxPool2DLayer(conv22, pool_size=pool)\n",
    "\n",
    "conv31 = layers.Conv2DLayer(pool2, num_filters=num3, filter_size=conv)\n",
    "conv32 = layers.Conv2DLayer(conv31, num_filters=num3, filter_size=conv)\n",
    "pool3 = layers.MaxPool2DLayer(conv32, pool_size=pool)\n",
    "\n",
    "hidden1 = layers.DenseLayer(layers.dropout(pool3, p_drop), num_units=200)\n",
    "hidden2 = layers.DenseLayer(layers.dropout(hidden1, p_drop), num_units=200)\n",
    "hidden3 = layers.DenseLayer(layers.dropout(hidden2, p_drop), num_units=50)\n",
    "\n",
    "network = layers.DenseLayer(hidden3, num_units=4, nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/Lasagne-0.1dev-py2.7.egg/lasagne/layers/helper.py:69: UserWarning: get_all_layers() has been changed to return layers in topological order. The former implementation is still available as get_all_layers_old(), but will be removed before the first release of Lasagne. To ignore this warning, use `warnings.filterwarnings('ignore', '.*topo.*')`.\n",
      "  warnings.warn(\"get_all_layers() has been changed to return layers in \"\n"
     ]
    }
   ],
   "source": [
    "target_var = T.ivector('targets') #The classes 0..9\n",
    "prediction = layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss = loss.mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.01, momentum=0.9)\n",
    "#print(\"Number of Parameters in network: {}\".format(len(params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var), dtype=theano.config.floatX)\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as imgplot\n",
    "import numpy as np\n",
    "from skimage import transform as tf\n",
    "\n",
    "#rots = np.deg2rad(np.asarray((90,180,0,5,-5,10,-10)))\n",
    "rots = np.deg2rad(range(0,359))\n",
    "\n",
    "dists = (-5,5)\n",
    "\n",
    "def manipulateTrainingData(Xb):\n",
    "    retX = np.zeros((Xb.shape[0], Xb.shape[1], Xb.shape[2], Xb.shape[3]), dtype='float32')\n",
    "    for i in range(len(Xb)):\n",
    "        rot = rots[np.random.randint(0, len(rots))]\n",
    "\n",
    "        tf_rotate = tf.SimilarityTransform(rotation=rot)\n",
    "        shift_y, shift_x = np.array((X.shape[2], X.shape[3])) / 2.\n",
    "        tf_shift = tf.SimilarityTransform(translation=[-shift_x, -shift_y])\n",
    "        tf_shift_inv = tf.SimilarityTransform(translation=[shift_x, shift_y])\n",
    "        tform_rot = (tf_shift + (tf_rotate + tf_shift_inv))\n",
    "\n",
    "        ## TODO add the transformations\n",
    "        scale = np.random.uniform(0.9,1.10)\n",
    "        d = tf.SimilarityTransform(scale=scale, translation=(np.random.randint(5),np.random.randint(5)))\n",
    "        tform_other = (tform_rot + d)\n",
    "\n",
    "        for c in range(np.shape(X)[1]):\n",
    "            maxAbs = 256.0;np.max(np.abs(Xb[i,c,:,:]))\n",
    "            # Needs at lease 0.11.3\n",
    "            retX[i,c,:,:] = tf.warp(Xb[i,c,:,:], tform_other, preserve_range = True) # \"Float Images\" are only allowed to have values between -1 and 1\n",
    "    return retX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################## Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order. It assumes\n",
    "# data is available as numpy arrays. For big datasets, you could load numpy\n",
    "# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your\n",
    "# own custom data iteration function. For small datasets, you can also copy\n",
    "# them to GPU at once for slightly improved performance. This would involve\n",
    "# several changes in the main program, though, and is not demonstrated here.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "perf = pd.DataFrame(columns=['train_loss','valid_loss','valid_accuracy', 'time'])\n",
    "perf_test = pd.DataFrame(columns=['epoch','valid_acc_mean','valid_acc_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40076, 5, 72, 72), (40076,), (10020, 5, 72, 72), (10020,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_train = int(X_train.shape[0] * 0.8)\n",
    "X_train1 = X_train[0:N_train]\n",
    "y_train1 = Y_train[0:N_train]\n",
    "X_val = X_train[N_train:]\n",
    "y_val = Y_train[N_train:]\n",
    "np.shape(X_train1), np.shape(y_train1), np.shape(X_val), np.shape(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_func = theano.function([input_var],[test_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "# We iterate over epochs:\n",
    "num_epochs = 510\n",
    "print('Starting')\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    print('Starting')\n",
    "    for batch in iterate_minibatches(X_train1, y_train1, 100, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        #print('Manipulating inputs '.format(np.shape(inputs)))\n",
    "        dd = manipulateTrainingData(inputs)\n",
    "        train_err += train_fn(dd, targets)\n",
    "        train_batches += 1\n",
    "    \n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_val, y_val, 50, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "    \n",
    "    time_taken = time.time() - start_time\n",
    "    perf.loc[epoch] = [train_err / train_batches, val_err / train_batches, val_acc / val_batches, time_taken]\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time_taken))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / train_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(val_acc / val_batches * 100))\n",
    "    \n",
    "    perf.to_csv('/home/dueo/Dropbox/Server_Sync/current_training.csv')\n",
    "    \n",
    "    ## Testing on the testset\n",
    "    avg = []\n",
    "    for batch in iterate_minibatches(X_test, Y_test, 100, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        res = pred_func(inputs)\n",
    "        avg.append(np.mean(np.argmax(res[0],axis=1) == targets))\n",
    "        perf_test.loc[epoch] = [epoch, np.mean(avg), np.std(avg)]\n",
    "    perf_test.to_csv('/home/dueo/Dropbox/Server_Sync/current_test.csv')\n",
    "    \n",
    "    # Optionally, you could now dump the network weights to a file like this:\n",
    "    # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
    "    #\n",
    "    # And load them again later on like this:\n",
    "    # with np.load('model.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # lasagne.layers.set_all_param_values(network, param_values)\n",
    "    if (epoch % 10 == 0):\n",
    "        np.savez('net_PAPER_aug_epoch{}_72x72large_net.pickle'.format(epoch), 'wb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
